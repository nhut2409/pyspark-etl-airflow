[2024-02-26T14:03:23.047+0700] {processor.py:161} INFO - Started process (PID=65395) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:03:23.048+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:03:23.048+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:03:23.048+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:03:38.071+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:03:38.070+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 9, in extract_data
    with zipfile.ZipFile('airflow/dags/data/brazilian-ecommerce.zip', 'r') as zip_ref:
  File "/usr/lib/python3.10/zipfile.py", line 1251, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: 'airflow/dags/data/brazilian-ecommerce.zip'
[2024-02-26T14:03:38.072+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:03:38.096+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.053 seconds
[2024-02-26T14:04:08.145+0700] {processor.py:161} INFO - Started process (PID=66732) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:04:08.147+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:04:08.147+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:04:08.147+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:04:23.431+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:04:23.430+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 9, in extract_data
    with zipfile.ZipFile('airflow/dags/data/brazilian-ecommerce.zip', 'r') as zip_ref:
  File "/usr/lib/python3.10/zipfile.py", line 1251, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: 'airflow/dags/data/brazilian-ecommerce.zip'
[2024-02-26T14:04:23.432+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:04:23.460+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.317 seconds
[2024-02-26T14:04:44.963+0700] {processor.py:161} INFO - Started process (PID=67864) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:04:44.964+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:04:44.965+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:04:44.965+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:04:44.966+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:04:44.966+0700] {dagbag.py:325} INFO - File /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py assumed to contain no DAGs. Skipping.
[2024-02-26T14:04:44.967+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:04:44.985+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 0.027 seconds
[2024-02-26T14:05:15.815+0700] {processor.py:161} INFO - Started process (PID=68420) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:05:15.816+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:05:15.816+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:05:15.816+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:05:15.817+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:05:15.817+0700] {dagbag.py:325} INFO - File /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py assumed to contain no DAGs. Skipping.
[2024-02-26T14:05:15.817+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:05:15.835+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 0.025 seconds
[2024-02-26T14:05:44.778+0700] {processor.py:161} INFO - Started process (PID=69214) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:05:44.779+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:05:44.780+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:05:44.779+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:05:59.841+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:05:59.840+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 9, in extract_data
    with zipfile.ZipFile('airflow/dags/pipeline/data', 'r') as zip_ref:
  File "/usr/lib/python3.10/zipfile.py", line 1251, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: 'airflow/dags/pipeline/data'
[2024-02-26T14:05:59.842+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:05:59.868+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.093 seconds
[2024-02-26T14:06:00.824+0700] {processor.py:161} INFO - Started process (PID=69839) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:00.825+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:06:00.826+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:06:00.826+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:16.140+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:06:16.138+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 9, in extract_data
    with zipfile.ZipFile('airflow/dags/pipeline/data/brazilian-ecommerce.zip', 'r') as zip_ref:
  File "/usr/lib/python3.10/zipfile.py", line 1251, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: 'airflow/dags/pipeline/data'
[2024-02-26T14:06:16.140+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:16.162+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.340 seconds
[2024-02-26T14:06:20.955+0700] {processor.py:161} INFO - Started process (PID=70810) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:20.956+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:06:20.956+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:06:20.956+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:20.957+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:06:20.957+0700] {dagbag.py:325} INFO - File /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py assumed to contain no DAGs. Skipping.
[2024-02-26T14:06:20.957+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:20.979+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 0.027 seconds
[2024-02-26T14:06:24.009+0700] {processor.py:161} INFO - Started process (PID=71006) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:24.009+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:06:24.010+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:06:24.010+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:24.011+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:06:24.011+0700] {dagbag.py:325} INFO - File /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py assumed to contain no DAGs. Skipping.
[2024-02-26T14:06:24.011+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:24.030+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 0.024 seconds
[2024-02-26T14:06:54.122+0700] {processor.py:161} INFO - Started process (PID=71209) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:54.123+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:06:54.125+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:06:54.125+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:54.126+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:06:54.126+0700] {dagbag.py:325} INFO - File /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py assumed to contain no DAGs. Skipping.
[2024-02-26T14:06:54.126+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:06:54.147+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 0.028 seconds
[2024-02-26T14:07:24.252+0700] {processor.py:161} INFO - Started process (PID=71959) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:07:24.253+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:07:24.254+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:07:24.254+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:07:24.255+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:07:24.255+0700] {dagbag.py:325} INFO - File /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py assumed to contain no DAGs. Skipping.
[2024-02-26T14:07:24.255+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:07:24.273+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 0.023 seconds
[2024-02-26T14:07:54.370+0700] {processor.py:161} INFO - Started process (PID=72621) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:07:54.371+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:07:54.372+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:07:54.372+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:07:54.373+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:07:54.373+0700] {dagbag.py:325} INFO - File /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py assumed to contain no DAGs. Skipping.
[2024-02-26T14:07:54.373+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:07:54.391+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 0.024 seconds
[2024-02-26T14:08:13.443+0700] {processor.py:161} INFO - Started process (PID=72807) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:08:13.444+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:08:13.445+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:08:13.444+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:08:13.445+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:08:13.445+0700] {dagbag.py:325} INFO - File /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py assumed to contain no DAGs. Skipping.
[2024-02-26T14:08:13.446+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:08:13.465+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 0.024 seconds
[2024-02-26T14:13:23.968+0700] {processor.py:161} INFO - Started process (PID=79336) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:13:23.969+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:13:23.970+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:13:23.970+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:13:39.425+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:13:39.424+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 9, in extract_data
    with zipfile.ZipFile('/airflow/dags/pipeline/data/brazilian-ecommerce.zip', 'r') as zip_ref:
  File "/usr/lib/python3.10/zipfile.py", line 1251, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: '/airflow/dags/pipeline/data/brazilian-ecommerce.zip'
[2024-02-26T14:13:39.425+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:13:39.450+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.484 seconds
[2024-02-26T14:14:09.621+0700] {processor.py:161} INFO - Started process (PID=80751) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:14:09.622+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:14:09.623+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:14:09.622+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:14:24.672+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:14:24.671+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 9, in extract_data
    with zipfile.ZipFile('/airflow/dags/pipeline/data/brazilian-ecommerce.zip', 'r') as zip_ref:
  File "/usr/lib/python3.10/zipfile.py", line 1251, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: '/airflow/dags/pipeline/data/brazilian-ecommerce.zip'
[2024-02-26T14:14:24.672+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:14:24.694+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.075 seconds
[2024-02-26T14:14:55.519+0700] {processor.py:161} INFO - Started process (PID=82125) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:14:55.520+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:14:55.520+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:14:55.520+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:15:10.601+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:15:10.600+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 9, in extract_data
    with zipfile.ZipFile('/airflow/dags/pipeline/data/brazilian-ecommerce.zip', 'r') as zip_ref:
  File "/usr/lib/python3.10/zipfile.py", line 1251, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: '/airflow/dags/pipeline/data/brazilian-ecommerce.zip'
[2024-02-26T14:15:10.602+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:15:10.631+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.116 seconds
[2024-02-26T14:15:41.044+0700] {processor.py:161} INFO - Started process (PID=83453) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:15:41.045+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:15:41.045+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:15:41.045+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:15:56.089+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:15:56.088+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 9, in extract_data
    with zipfile.ZipFile('/airflow/dags/pipeline/data/brazilian-ecommerce.zip', 'r') as zip_ref:
  File "/usr/lib/python3.10/zipfile.py", line 1251, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: '/airflow/dags/pipeline/data/brazilian-ecommerce.zip'
[2024-02-26T14:15:56.090+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:15:56.114+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.073 seconds
[2024-02-26T14:16:26.293+0700] {processor.py:161} INFO - Started process (PID=84784) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:16:26.294+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:16:26.295+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:16:26.295+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:16:41.137+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:16:41.137+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 9, in extract_data
    with zipfile.ZipFile('/airflow/dags/pipeline/data/brazilian-ecommerce.zip', 'r') as zip_ref:
  File "/usr/lib/python3.10/zipfile.py", line 1251, in __init__
    self.fp = io.open(file, filemode)
FileNotFoundError: [Errno 2] No such file or directory: '/airflow/dags/pipeline/data/brazilian-ecommerce.zip'
[2024-02-26T14:16:41.138+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:16:41.167+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.876 seconds
[2024-02-26T14:16:51.977+0700] {processor.py:161} INFO - Started process (PID=85742) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:16:51.978+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:16:51.978+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:16:51.978+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:16:51.979+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:16:51.979+0700] {dagbag.py:325} INFO - File /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py assumed to contain no DAGs. Skipping.
[2024-02-26T14:16:51.979+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:16:51.999+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 0.025 seconds
[2024-02-26T14:39:26.491+0700] {processor.py:161} INFO - Started process (PID=8495) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:39:26.492+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:39:26.493+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:39:26.493+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:39:56.495+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:39:56.495+0700] {timeout.py:68} ERROR - Process timed out, PID: 8495
[2024-02-26T14:39:56.496+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:39:56.496+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 8495
[2024-02-26T14:39:56.497+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:39:56.497+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:39:56.498+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:39:56.497+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 8495

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T14:39:56.498+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:39:56.498+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:39:56.499+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:39:56.498+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o35.parquet
[2024-02-26T14:39:56.500+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:39:56.528+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.039 seconds
[2024-02-26T14:40:27.481+0700] {processor.py:161} INFO - Started process (PID=10224) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:40:27.482+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:40:27.483+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:40:27.482+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:40:57.486+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:40:57.485+0700] {timeout.py:68} ERROR - Process timed out, PID: 10224
[2024-02-26T14:40:57.486+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:40:57.486+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 10224
[2024-02-26T14:40:57.487+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:40:57.487+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:40:57.488+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:40:57.487+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 10224

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T14:40:57.488+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:40:57.488+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:40:57.489+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:40:57.488+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o35.parquet
[2024-02-26T14:40:57.489+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:40:57.516+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.042 seconds
[2024-02-26T14:41:28.520+0700] {processor.py:161} INFO - Started process (PID=11778) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:41:28.521+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:41:28.522+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:41:28.522+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:41:58.524+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:41:58.524+0700] {timeout.py:68} ERROR - Process timed out, PID: 11778
[2024-02-26T14:41:58.525+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:41:58.524+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 11778
[2024-02-26T14:41:58.525+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:41:58.525+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:41:58.526+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:41:58.525+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 11778

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T14:41:58.526+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:41:58.526+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:41:58.527+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:41:58.526+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o35.parquet
[2024-02-26T14:41:58.527+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:41:58.551+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.034 seconds
[2024-02-26T14:42:29.419+0700] {processor.py:161} INFO - Started process (PID=13239) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:42:29.420+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:42:29.420+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:42:29.420+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:42:59.422+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:42:59.422+0700] {timeout.py:68} ERROR - Process timed out, PID: 13239
[2024-02-26T14:42:59.423+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:42:59.422+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 13239
[2024-02-26T14:42:59.423+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:42:59.423+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:42:59.424+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:42:59.424+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 13239

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T14:42:59.424+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:42:59.424+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:42:59.425+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:42:59.424+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o35.parquet
[2024-02-26T14:42:59.425+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:42:59.449+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.033 seconds
[2024-02-26T14:43:30.416+0700] {processor.py:161} INFO - Started process (PID=14818) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:43:30.417+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:43:30.418+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:43:30.417+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:44:00.419+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:44:00.419+0700] {timeout.py:68} ERROR - Process timed out, PID: 14818
[2024-02-26T14:44:00.420+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:44:00.420+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 14818
[2024-02-26T14:44:00.421+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:44:00.421+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:44:00.421+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:44:00.421+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 14818

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T14:44:00.422+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:44:00.421+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:44:00.422+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:44:00.422+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o35.parquet
[2024-02-26T14:44:00.423+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:44:00.447+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.034 seconds
[2024-02-26T14:44:31.056+0700] {processor.py:161} INFO - Started process (PID=16365) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:44:31.057+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:44:31.058+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:44:31.058+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:45:01.060+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:45:01.060+0700] {timeout.py:68} ERROR - Process timed out, PID: 16365
[2024-02-26T14:45:01.061+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:45:01.061+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 16365
[2024-02-26T14:45:01.062+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:45:01.061+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:45:01.062+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:45:01.062+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 16365

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T14:45:01.063+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:45:01.063+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:45:01.064+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:45:01.063+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o35.parquet
[2024-02-26T14:45:01.064+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:45:01.091+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.038 seconds
[2024-02-26T14:45:31.208+0700] {processor.py:161} INFO - Started process (PID=17775) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:45:31.209+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:45:31.210+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:45:31.210+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:46:01.213+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:46:01.212+0700] {timeout.py:68} ERROR - Process timed out, PID: 17775
[2024-02-26T14:46:01.215+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:46:01.213+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 17775
[2024-02-26T14:46:01.216+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:46:01.215+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:46:01.217+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:46:01.216+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 17775

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T14:46:01.217+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:46:01.217+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:46:01.219+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:46:01.218+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o35.parquet
[2024-02-26T14:46:01.220+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:46:01.273+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.068 seconds
[2024-02-26T14:46:31.917+0700] {processor.py:161} INFO - Started process (PID=19469) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:46:31.919+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:46:31.919+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:46:31.919+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:53:42.605+0700] {processor.py:161} INFO - Started process (PID=22063) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:53:42.606+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:53:42.606+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:53:42.606+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:54:12.608+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:54:12.608+0700] {timeout.py:68} ERROR - Process timed out, PID: 22063
[2024-02-26T14:54:12.610+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:54:12.609+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 22063
[2024-02-26T14:54:12.610+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:54:12.610+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:54:12.611+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:54:12.610+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 22063

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T14:54:12.611+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:54:12.611+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:54:12.612+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:54:12.612+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o40.parquet
[2024-02-26T14:54:12.613+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:54:12.641+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.039 seconds
[2024-02-26T14:54:43.532+0700] {processor.py:161} INFO - Started process (PID=23553) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:54:43.533+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:54:43.533+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:54:43.533+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:55:13.535+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:55:13.535+0700] {timeout.py:68} ERROR - Process timed out, PID: 23553
[2024-02-26T14:55:13.536+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:55:13.535+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 23553
[2024-02-26T14:55:13.536+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:55:13.536+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:55:13.537+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:55:13.536+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 23553

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T14:55:13.537+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:55:13.537+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T14:55:13.538+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:55:13.537+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o40.parquet
[2024-02-26T14:55:13.538+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:55:13.560+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.031 seconds
[2024-02-26T14:55:44.469+0700] {processor.py:161} INFO - Started process (PID=25169) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:55:44.469+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T14:55:44.470+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:55:44.470+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T14:55:44.898+0700] {logging_mixin.py:188} INFO - [2024-02-26T14:55:44.897+0700] {process_utils.py:262} INFO - Waiting up to 5 seconds for processes to exit...
[2024-02-26T15:01:44.765+0700] {processor.py:161} INFO - Started process (PID=26606) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:01:44.766+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:01:44.767+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:01:44.767+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:02:14.769+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:02:14.769+0700] {timeout.py:68} ERROR - Process timed out, PID: 26606
[2024-02-26T15:02:14.770+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:02:14.770+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 26606
[2024-02-26T15:02:14.770+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:02:14.770+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:02:14.771+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:02:14.771+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 26606

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:02:14.771+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:02:14.771+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:02:14.772+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:02:14.771+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o40.parquet
[2024-02-26T15:02:14.772+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:02:14.798+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.036 seconds
[2024-02-26T15:02:45.736+0700] {processor.py:161} INFO - Started process (PID=28167) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:02:45.737+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:02:45.738+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:02:45.738+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:03:15.739+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:03:15.739+0700] {timeout.py:68} ERROR - Process timed out, PID: 28167
[2024-02-26T15:03:15.740+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:03:15.740+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 28167
[2024-02-26T15:03:15.741+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:03:15.741+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:03:15.741+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:03:15.741+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 28167

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:03:15.742+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:03:15.742+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:03:15.742+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:03:15.742+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o40.parquet
[2024-02-26T15:03:15.743+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:03:15.766+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.033 seconds
[2024-02-26T15:03:46.650+0700] {processor.py:161} INFO - Started process (PID=29742) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:03:46.651+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:03:46.652+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:03:46.651+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:04:16.653+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:04:16.653+0700] {timeout.py:68} ERROR - Process timed out, PID: 29742
[2024-02-26T15:04:16.654+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:04:16.653+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 29742
[2024-02-26T15:04:16.654+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:04:16.654+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:04:16.655+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:04:16.654+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 29742

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:04:16.655+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:04:16.655+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:04:16.656+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:04:16.655+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o40.parquet
[2024-02-26T15:04:16.656+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:04:16.679+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.031 seconds
[2024-02-26T15:04:47.586+0700] {processor.py:161} INFO - Started process (PID=31172) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:04:47.587+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:04:47.587+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:04:47.587+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:05:17.590+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:05:17.589+0700] {timeout.py:68} ERROR - Process timed out, PID: 31172
[2024-02-26T15:05:17.591+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:05:17.590+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 31172
[2024-02-26T15:05:17.592+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:05:17.592+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:05:17.595+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:05:17.593+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 31172

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:05:17.595+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:05:17.595+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:05:17.597+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:05:17.595+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o40.parquet
[2024-02-26T15:05:17.597+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:05:17.686+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.103 seconds
[2024-02-26T15:05:48.626+0700] {processor.py:161} INFO - Started process (PID=32720) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:05:48.626+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:05:48.627+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:05:48.627+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:06:18.628+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:06:18.628+0700] {timeout.py:68} ERROR - Process timed out, PID: 32720
[2024-02-26T15:06:18.629+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:06:18.629+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 32720
[2024-02-26T15:06:18.629+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:06:18.629+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:06:18.630+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:06:18.630+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 32720

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:06:18.630+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:06:18.630+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:06:18.631+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:06:18.630+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o40.parquet
[2024-02-26T15:06:18.631+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:06:18.654+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.031 seconds
[2024-02-26T15:06:50.247+0700] {processor.py:161} INFO - Started process (PID=34311) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:06:50.248+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:06:50.248+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:06:50.248+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:07:20.251+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:20.251+0700] {timeout.py:68} ERROR - Process timed out, PID: 34311
[2024-02-26T15:07:20.253+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:20.252+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 34311
[2024-02-26T15:07:20.253+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:20.253+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:07:20.255+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:20.254+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 34311

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:07:20.255+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:20.255+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:07:20.257+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:20.256+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o40.parquet
[2024-02-26T15:07:20.258+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:07:20.302+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.058 seconds
[2024-02-26T15:07:29.340+0700] {processor.py:161} INFO - Started process (PID=35757) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:07:29.341+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:07:29.341+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:29.341+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:07:59.343+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:59.343+0700] {timeout.py:68} ERROR - Process timed out, PID: 35757
[2024-02-26T15:07:59.344+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:59.343+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 35757
[2024-02-26T15:07:59.344+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:59.344+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:07:59.345+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:59.345+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 35757

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:07:59.345+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:59.345+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:07:59.346+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:07:59.345+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:07:59.346+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:07:59.369+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.031 seconds
[2024-02-26T15:08:29.514+0700] {processor.py:161} INFO - Started process (PID=37871) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:08:29.515+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:08:29.515+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:08:29.515+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:08:59.517+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:08:59.517+0700] {timeout.py:68} ERROR - Process timed out, PID: 37871
[2024-02-26T15:08:59.518+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:08:59.518+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 37871
[2024-02-26T15:08:59.519+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:08:59.519+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:08:59.520+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:08:59.519+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 37871

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:08:59.520+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:08:59.520+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:08:59.520+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:08:59.520+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:08:59.521+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:08:59.544+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.032 seconds
[2024-02-26T15:11:45.667+0700] {processor.py:161} INFO - Started process (PID=40531) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:11:45.668+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:11:45.669+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:11:45.669+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:12:01.803+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:12:01.830+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 16.165 seconds
[2024-02-26T15:12:32.668+0700] {processor.py:161} INFO - Started process (PID=41443) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:12:32.669+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:12:32.670+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:12:32.670+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:12:48.117+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:12:48.139+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.473 seconds
[2024-02-26T15:13:18.971+0700] {processor.py:161} INFO - Started process (PID=42474) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:13:18.972+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:13:18.973+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:13:18.972+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:13:34.515+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:13:34.536+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.569 seconds
[2024-02-26T15:14:05.373+0700] {processor.py:161} INFO - Started process (PID=43577) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:14:05.374+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:14:05.375+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:14:05.375+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:14:21.476+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:14:21.497+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 16.127 seconds
[2024-02-26T15:14:52.282+0700] {processor.py:161} INFO - Started process (PID=44836) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:14:52.283+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:14:52.283+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:14:52.283+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:15:22.285+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:15:22.285+0700] {timeout.py:68} ERROR - Process timed out, PID: 44836
[2024-02-26T15:15:22.286+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:15:22.286+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 44836
[2024-02-26T15:15:22.287+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:15:22.287+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:15:22.287+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:15:22.287+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 44836

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:15:22.288+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:15:22.287+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:15:22.288+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:15:22.288+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:15:22.289+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:15:22.318+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.039 seconds
[2024-02-26T15:15:53.188+0700] {processor.py:161} INFO - Started process (PID=46533) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:15:53.189+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:15:53.190+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:15:53.190+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:16:23.192+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:16:23.191+0700] {timeout.py:68} ERROR - Process timed out, PID: 46533
[2024-02-26T15:16:23.193+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:16:23.192+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 46533
[2024-02-26T15:16:23.193+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:16:23.193+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:16:23.194+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:16:23.193+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 46533

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:16:23.194+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:16:23.194+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:16:23.195+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:16:23.194+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:16:23.195+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:16:23.217+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.033 seconds
[2024-02-26T15:16:53.725+0700] {processor.py:161} INFO - Started process (PID=48271) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:16:53.726+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:16:53.726+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:16:53.726+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:17:23.728+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:17:23.728+0700] {timeout.py:68} ERROR - Process timed out, PID: 48271
[2024-02-26T15:17:23.729+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:17:23.728+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 48271
[2024-02-26T15:17:23.729+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:17:23.729+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:17:23.730+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:17:23.729+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 48271

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:17:23.730+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:17:23.730+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:17:23.731+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:17:23.730+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:17:23.731+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:17:23.753+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.030 seconds
[2024-02-26T15:24:38.243+0700] {processor.py:161} INFO - Started process (PID=52619) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:24:38.243+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:24:38.244+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:24:38.244+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:25:08.246+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:25:08.246+0700] {timeout.py:68} ERROR - Process timed out, PID: 52619
[2024-02-26T15:25:08.247+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:25:08.246+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 52619
[2024-02-26T15:25:08.247+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:25:08.247+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:25:08.248+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:25:08.247+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 52619

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:25:08.248+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:25:08.248+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:25:08.249+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:25:08.248+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:25:08.249+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:25:08.272+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.031 seconds
[2024-02-26T15:25:39.102+0700] {processor.py:161} INFO - Started process (PID=54121) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:25:39.103+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:25:39.103+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:25:39.103+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:26:09.106+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:26:09.105+0700] {timeout.py:68} ERROR - Process timed out, PID: 54121
[2024-02-26T15:26:09.108+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:26:09.107+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 54121
[2024-02-26T15:26:09.108+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:26:09.108+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:26:09.109+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:26:09.108+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 54121

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:26:09.110+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:26:09.110+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:26:09.112+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:26:09.111+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:26:09.113+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:26:09.173+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.074 seconds
[2024-02-26T15:26:40.135+0700] {processor.py:161} INFO - Started process (PID=55674) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:26:40.136+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:26:40.136+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:26:40.136+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:27:10.138+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:27:10.138+0700] {timeout.py:68} ERROR - Process timed out, PID: 55674
[2024-02-26T15:27:10.139+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:27:10.138+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 55674
[2024-02-26T15:27:10.140+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:27:10.139+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:27:10.140+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:27:10.140+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 55674

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:27:10.141+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:27:10.141+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:27:10.142+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:27:10.141+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:27:10.142+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:27:10.188+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.056 seconds
[2024-02-26T15:37:29.066+0700] {processor.py:161} INFO - Started process (PID=59353) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:37:29.067+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:37:29.068+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:37:29.067+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:37:59.069+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:37:59.069+0700] {timeout.py:68} ERROR - Process timed out, PID: 59353
[2024-02-26T15:37:59.070+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:37:59.069+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 59353
[2024-02-26T15:37:59.070+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:37:59.070+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:37:59.071+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:37:59.070+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 59353

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:37:59.071+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:37:59.071+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:37:59.072+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:37:59.071+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:37:59.072+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:37:59.095+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.031 seconds
[2024-02-26T15:38:29.917+0700] {processor.py:161} INFO - Started process (PID=60830) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:38:29.918+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:38:29.918+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:38:29.918+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:38:59.921+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:38:59.920+0700] {timeout.py:68} ERROR - Process timed out, PID: 60830
[2024-02-26T15:38:59.922+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:38:59.921+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 60830
[2024-02-26T15:38:59.922+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:38:59.922+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:38:59.923+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:38:59.922+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 60830

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:38:59.923+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:38:59.923+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:38:59.924+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:38:59.924+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:38:59.925+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:38:59.953+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.039 seconds
[2024-02-26T15:39:30.894+0700] {processor.py:161} INFO - Started process (PID=62388) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:39:30.895+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:39:30.895+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:39:30.895+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:40:00.897+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:40:00.897+0700] {timeout.py:68} ERROR - Process timed out, PID: 62388
[2024-02-26T15:40:00.898+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:40:00.898+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 62388
[2024-02-26T15:40:00.898+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:40:00.898+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:40:00.899+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:40:00.899+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 62388

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:40:00.899+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:40:00.899+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:40:00.900+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:40:00.899+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:40:00.901+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:40:00.924+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.033 seconds
[2024-02-26T15:40:31.776+0700] {processor.py:161} INFO - Started process (PID=63815) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:40:31.776+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:40:31.777+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:40:31.776+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:41:01.779+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:41:01.778+0700] {timeout.py:68} ERROR - Process timed out, PID: 63815
[2024-02-26T15:41:01.780+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:41:01.779+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 63815
[2024-02-26T15:41:01.781+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:41:01.781+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:41:01.781+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:41:01.781+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 63815

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:41:01.782+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:41:01.781+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:41:01.782+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:41:01.782+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:41:01.783+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:41:01.811+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.038 seconds
[2024-02-26T15:41:32.701+0700] {processor.py:161} INFO - Started process (PID=65357) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:41:32.702+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:41:32.702+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:41:32.702+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:42:02.704+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:42:02.704+0700] {timeout.py:68} ERROR - Process timed out, PID: 65357
[2024-02-26T15:42:02.705+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:42:02.704+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 65357
[2024-02-26T15:42:02.705+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:42:02.705+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:42:02.706+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:42:02.705+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 65357

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:42:02.706+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:42:02.706+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:42:02.707+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:42:02.706+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:42:02.707+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:42:02.736+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.037 seconds
[2024-02-26T15:42:34.406+0700] {processor.py:161} INFO - Started process (PID=66946) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:42:34.407+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:42:34.408+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:42:34.408+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:43:04.411+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:43:04.410+0700] {timeout.py:68} ERROR - Process timed out, PID: 66946
[2024-02-26T15:43:04.412+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:43:04.411+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 66946
[2024-02-26T15:43:04.412+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:43:04.412+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:43:04.413+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:43:04.413+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 66946

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:43:04.414+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:43:04.413+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:43:04.415+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:43:04.414+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:43:04.415+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:43:04.441+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.038 seconds
[2024-02-26T15:43:34.541+0700] {processor.py:161} INFO - Started process (PID=68524) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:43:34.542+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:43:34.543+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:43:34.543+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:44:00.228+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:44:00.226+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o41.parquet.
: org.apache.spark.SparkException: Unable to clear output directory s3a://olistsalesbucket/olist_order_items_dataset.parquet prior to writing to it.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotClearOutputDirectoryError(QueryExecutionErrors.scala:794)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:239)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:131)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:44:00.229+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:44:00.259+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 25.722 seconds
[2024-02-26T15:44:30.697+0700] {processor.py:161} INFO - Started process (PID=70608) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:44:30.698+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:44:30.698+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:44:30.698+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:45:00.700+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:00.700+0700] {timeout.py:68} ERROR - Process timed out, PID: 70608
[2024-02-26T15:45:00.701+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:00.700+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 70608
[2024-02-26T15:45:00.702+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:00.701+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:45:00.702+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:00.702+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 70608

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:45:00.703+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:00.703+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:45:00.704+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:00.703+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:45:00.704+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:45:00.733+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.038 seconds
[2024-02-26T15:45:23.467+0700] {processor.py:161} INFO - Started process (PID=72451) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:45:23.468+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T15:45:23.469+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:23.469+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:45:53.470+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:53.470+0700] {timeout.py:68} ERROR - Process timed out, PID: 72451
[2024-02-26T15:45:53.471+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:53.471+0700] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 72451
[2024-02-26T15:45:53.471+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:53.471+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:45:53.472+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:53.472+0700] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 72451

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-26T15:45:53.472+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:53.472+0700] {clientserver.py:543} INFO - Closing down clientserver connection
[2024-02-26T15:45:53.473+0700] {logging_mixin.py:188} INFO - [2024-02-26T15:45:53.473+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 27, in <module>
    extract_data()
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 25, in extract_data
    df.coalesce(1).write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
    self._jwrite.parquet(path)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o41.parquet
[2024-02-26T15:45:53.473+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T15:45:53.495+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.030 seconds
[2024-02-26T16:03:23.211+0700] {processor.py:161} INFO - Started process (PID=3973) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:03:23.212+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:03:23.213+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:03:23.213+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:03:39.072+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:03:39.091+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.882 seconds
[2024-02-26T16:04:09.847+0700] {processor.py:161} INFO - Started process (PID=4706) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:04:09.848+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:04:09.848+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:04:09.848+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:04:24.339+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:04:24.355+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.510 seconds
[2024-02-26T16:04:55.164+0700] {processor.py:161} INFO - Started process (PID=5546) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:04:55.165+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:04:55.165+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:04:55.165+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:05:09.424+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:05:09.442+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.282 seconds
[2024-02-26T16:05:33.747+0700] {processor.py:161} INFO - Started process (PID=6716) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:05:33.748+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:05:33.749+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:05:33.749+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:05:48.288+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:05:48.305+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.560 seconds
[2024-02-26T16:06:19.085+0700] {processor.py:161} INFO - Started process (PID=7579) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:06:19.086+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:06:19.087+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:06:19.086+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:06:49.088+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:06:49.088+0700] {timeout.py:68} ERROR - Process timed out, PID: 7579
[2024-02-26T16:06:49.091+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:06:49.089+0700] {dagbag.py:348} ERROR - Failed to import: /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py", line 5, in <module>
    from spark import spark
  File "/home/nhut/etl-airflow/airflow/dags/spark/__init__.py", line 1, in <module>
    from .spark_init import spark
  File "/home/nhut/etl-airflow/airflow/dags/spark/spark_init.py", line 4, in <module>
    spark = spark_config.spark_init(access_id, access_key)
  File "/home/nhut/etl-airflow/airflow/dags/spark/spark_config.py", line 11, in spark_init
    sc=pyspark.SparkContext()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/context.py", line 201, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/context.py", line 436, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/pyspark/java_gateway.py", line 104, in launch_gateway
    time.sleep(0.1)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.8.1/best-practices.html#reducing-dag-complexity, PID: 7579
[2024-02-26T16:06:49.091+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:06:49.119+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 30.037 seconds
[2024-02-26T16:08:08.273+0700] {processor.py:161} INFO - Started process (PID=8337) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:08:08.274+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:08:08.275+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:08:08.274+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:08:22.953+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:08:22.977+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.706 seconds
[2024-02-26T16:08:53.767+0700] {processor.py:161} INFO - Started process (PID=9192) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:08:53.768+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:08:53.769+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:08:53.768+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:09:08.167+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:09:08.184+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.419 seconds
[2024-02-26T16:09:39.055+0700] {processor.py:161} INFO - Started process (PID=10039) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:09:39.056+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:09:39.056+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:09:39.056+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:09:53.710+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:09:53.729+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.676 seconds
[2024-02-26T16:10:24.545+0700] {processor.py:161} INFO - Started process (PID=10897) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:10:24.546+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:10:24.546+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:10:24.546+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:10:38.861+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:10:38.878+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.336 seconds
[2024-02-26T16:17:57.151+0700] {processor.py:161} INFO - Started process (PID=14356) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:17:57.152+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:17:57.153+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:17:57.153+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:18:12.209+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:18:12.226+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.077 seconds
[2024-02-26T16:18:43.032+0700] {processor.py:161} INFO - Started process (PID=15483) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:18:43.033+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:18:43.033+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:18:43.033+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:18:57.805+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:18:57.823+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.794 seconds
[2024-02-26T16:19:27.878+0700] {processor.py:161} INFO - Started process (PID=16390) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:19:27.879+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:19:27.879+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:19:27.879+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:19:42.286+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:19:42.303+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.427 seconds
[2024-02-26T16:20:13.090+0700] {processor.py:161} INFO - Started process (PID=17308) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:20:13.091+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:20:13.092+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:20:13.092+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:20:27.624+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:20:27.661+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.574 seconds
[2024-02-26T16:20:58.465+0700] {processor.py:161} INFO - Started process (PID=18319) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:20:58.466+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:20:58.466+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:20:58.466+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:21:13.190+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:21:13.210+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.747 seconds
[2024-02-26T16:22:17.441+0700] {processor.py:161} INFO - Started process (PID=19937) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:22:17.442+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:22:17.443+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:22:17.443+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:22:32.649+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:22:32.668+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.230 seconds
[2024-02-26T16:23:03.482+0700] {processor.py:161} INFO - Started process (PID=21037) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:23:03.483+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:23:03.483+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:23:03.483+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:23:18.124+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:23:18.142+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.663 seconds
[2024-02-26T16:23:49.004+0700] {processor.py:161} INFO - Started process (PID=21979) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:23:49.004+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:23:49.005+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:23:49.004+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:24:03.527+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:24:03.547+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.546 seconds
[2024-02-26T16:24:34.467+0700] {processor.py:161} INFO - Started process (PID=23263) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:24:34.467+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:24:34.468+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:24:34.468+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:24:48.882+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:24:48.900+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.436 seconds
[2024-02-26T16:25:19.692+0700] {processor.py:161} INFO - Started process (PID=24237) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:25:19.693+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:25:19.693+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:25:19.693+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:25:34.207+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:25:34.228+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.538 seconds
[2024-02-26T16:26:05.038+0700] {processor.py:161} INFO - Started process (PID=25555) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:26:05.039+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:26:05.040+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:26:05.040+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:26:19.658+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:26:19.676+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.640 seconds
[2024-02-26T16:26:50.461+0700] {processor.py:161} INFO - Started process (PID=26503) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:26:50.462+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:26:50.463+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:26:50.463+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:27:05.122+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:27:05.141+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.683 seconds
[2024-02-26T16:27:35.682+0700] {processor.py:161} INFO - Started process (PID=27817) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:27:35.682+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:27:35.683+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:27:35.683+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:27:50.654+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:27:50.671+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.992 seconds
[2024-02-26T16:28:21.586+0700] {processor.py:161} INFO - Started process (PID=29111) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:28:21.587+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:28:21.587+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:28:21.587+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:28:36.086+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:28:36.106+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.522 seconds
[2024-02-26T16:29:07.028+0700] {processor.py:161} INFO - Started process (PID=30381) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:29:07.029+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:29:07.029+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:29:07.029+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:29:21.459+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:29:21.476+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.451 seconds
[2024-02-26T16:32:02.157+0700] {processor.py:161} INFO - Started process (PID=32290) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:32:02.158+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:32:02.158+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:32:02.158+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:32:16.836+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:32:16.853+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.699 seconds
[2024-02-26T16:32:47.633+0700] {processor.py:161} INFO - Started process (PID=33574) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:32:47.634+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:32:47.635+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:32:47.634+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:33:02.858+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:33:02.875+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 15.244 seconds
[2024-02-26T16:33:33.471+0700] {processor.py:161} INFO - Started process (PID=34420) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:33:33.472+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:33:33.473+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:33:33.473+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:33:47.901+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:33:47.920+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.451 seconds
[2024-02-26T16:34:18.691+0700] {processor.py:161} INFO - Started process (PID=35196) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:34:18.692+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:34:18.693+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:34:18.692+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:34:33.068+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:34:33.085+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.396 seconds
[2024-02-26T16:35:03.944+0700] {processor.py:161} INFO - Started process (PID=35947) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:35:03.945+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:35:03.945+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:35:03.945+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:35:18.598+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:35:18.616+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.675 seconds
[2024-02-26T16:35:49.525+0700] {processor.py:161} INFO - Started process (PID=37135) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:35:49.526+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:35:49.526+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:35:49.526+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:36:04.123+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:36:04.142+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.620 seconds
[2024-02-26T16:36:34.453+0700] {processor.py:161} INFO - Started process (PID=37852) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:36:34.454+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:36:34.454+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:36:34.454+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:36:49.356+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:36:49.375+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.925 seconds
[2024-02-26T16:37:20.105+0700] {processor.py:161} INFO - Started process (PID=39188) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:37:20.105+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:37:20.106+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:37:20.106+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:37:34.518+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:37:34.536+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.434 seconds
[2024-02-26T16:38:05.061+0700] {processor.py:161} INFO - Started process (PID=40080) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:38:05.062+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:38:05.063+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:38:05.062+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:38:19.735+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:38:19.753+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.694 seconds
[2024-02-26T16:38:50.712+0700] {processor.py:161} INFO - Started process (PID=41030) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:38:50.713+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:38:50.714+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:38:50.714+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:39:05.368+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:39:05.386+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.676 seconds
[2024-02-26T16:39:36.204+0700] {processor.py:161} INFO - Started process (PID=41882) to work on /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:39:36.205+0700] {processor.py:830} INFO - Processing file /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py for tasks to queue
[2024-02-26T16:39:36.205+0700] {logging_mixin.py:188} INFO - [2024-02-26T16:39:36.205+0700] {dagbag.py:538} INFO - Filling up the DagBag from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:39:50.668+0700] {processor.py:842} WARNING - No viable dags retrieved from /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py
[2024-02-26T16:39:50.686+0700] {processor.py:183} INFO - Processing /home/nhut/etl-airflow/airflow/dags/pipeline/spark_ingestion_to_s3.py took 14.485 seconds
