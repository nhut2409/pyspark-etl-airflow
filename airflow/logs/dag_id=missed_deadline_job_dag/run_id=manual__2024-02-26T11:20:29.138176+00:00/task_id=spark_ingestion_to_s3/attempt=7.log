[2024-02-27T12:07:12.350+0700] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: missed_deadline_job_dag.spark_ingestion_to_s3 manual__2024-02-26T11:20:29.138176+00:00 [queued]>
[2024-02-27T12:07:12.369+0700] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: missed_deadline_job_dag.spark_ingestion_to_s3 manual__2024-02-26T11:20:29.138176+00:00 [queued]>
[2024-02-27T12:07:12.370+0700] {taskinstance.py:2170} INFO - Starting attempt 7 of 7
[2024-02-27T12:07:12.450+0700] {taskinstance.py:2191} INFO - Executing <Task(BashOperator): spark_ingestion_to_s3> on 2024-02-26 11:20:29.138176+00:00
[2024-02-27T12:07:12.455+0700] {standard_task_runner.py:60} INFO - Started process 32116 to run task
[2024-02-27T12:07:12.463+0700] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'missed_deadline_job_dag', 'spark_ingestion_to_s3', 'manual__2024-02-26T11:20:29.138176+00:00', '--job-id', '144', '--raw', '--subdir', 'DAGS_FOLDER/missed_deadlinee_dag.py', '--cfg-path', '/tmp/tmpilt4nsvl']
[2024-02-27T12:07:12.465+0700] {standard_task_runner.py:88} INFO - Job 144: Subtask spark_ingestion_to_s3
[2024-02-27T12:07:12.664+0700] {task_command.py:423} INFO - Running <TaskInstance: missed_deadline_job_dag.spark_ingestion_to_s3 manual__2024-02-26T11:20:29.138176+00:00 [running]> on host MinhNhut.
[2024-02-27T12:07:12.979+0700] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='missed_deadline_job_dag' AIRFLOW_CTX_TASK_ID='spark_ingestion_to_s3' AIRFLOW_CTX_EXECUTION_DATE='2024-02-26T11:20:29.138176+00:00' AIRFLOW_CTX_TRY_NUMBER='7' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-02-26T11:20:29.138176+00:00'
[2024-02-27T12:07:12.981+0700] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2024-02-27T12:07:12.982+0700] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'python /home/nhut/etl-***/pipeline/spark_ingestion_to_s3.py']
[2024-02-27T12:07:12.999+0700] {subprocess.py:86} INFO - Output:
[2024-02-27T12:07:23.058+0700] {subprocess.py:93} INFO - 24/02/27 12:07:23 WARN Utils: Your hostname, MinhNhut resolves to a loopback address: 127.0.1.1; using 172.28.89.67 instead (on interface eth0)
[2024-02-27T12:07:23.076+0700] {subprocess.py:93} INFO - 24/02/27 12:07:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2024-02-27T12:07:33.361+0700] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/home/nhut/etl-***/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-02-27T12:07:33.479+0700] {subprocess.py:93} INFO - Ivy Default Cache set to: /home/nhut/.ivy2/cache
[2024-02-27T12:07:33.480+0700] {subprocess.py:93} INFO - The jars for the packages stored in: /home/nhut/.ivy2/jars
[2024-02-27T12:07:33.487+0700] {subprocess.py:93} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2024-02-27T12:07:33.488+0700] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-86f5bbc5-73d9-4a44-a372-fbd26883602d;1.0
[2024-02-27T12:07:33.489+0700] {subprocess.py:93} INFO - 	confs: [default]
[2024-02-27T12:07:33.794+0700] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-aws;3.3.2 in spark-list
[2024-02-27T12:07:33.932+0700] {subprocess.py:93} INFO - 	found com.amazonaws#aws-java-sdk-bundle;1.11.1026 in spark-list
[2024-02-27T12:07:34.032+0700] {subprocess.py:93} INFO - 	found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2024-02-27T12:07:34.106+0700] {subprocess.py:93} INFO - :: resolution report :: resolve 588ms :: artifacts dl 27ms
[2024-02-27T12:07:34.107+0700] {subprocess.py:93} INFO - 	:: modules in use:
[2024-02-27T12:07:34.108+0700] {subprocess.py:93} INFO - 	com.amazonaws#aws-java-sdk-bundle;1.11.1026 from spark-list in [default]
[2024-02-27T12:07:34.109+0700] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-aws;3.3.2 from spark-list in [default]
[2024-02-27T12:07:34.110+0700] {subprocess.py:93} INFO - 	org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2024-02-27T12:07:34.111+0700] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2024-02-27T12:07:34.112+0700] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2024-02-27T12:07:34.113+0700] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-02-27T12:07:34.114+0700] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2024-02-27T12:07:34.115+0700] {subprocess.py:93} INFO - 	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |
[2024-02-27T12:07:34.115+0700] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2024-02-27T12:07:34.139+0700] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-86f5bbc5-73d9-4a44-a372-fbd26883602d
[2024-02-27T12:07:34.141+0700] {subprocess.py:93} INFO - 	confs: [default]
[2024-02-27T12:07:34.201+0700] {subprocess.py:93} INFO - 	0 artifacts copied, 3 already retrieved (0kB/59ms)
[2024-02-27T12:07:35.685+0700] {subprocess.py:93} INFO - 24/02/27 12:07:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-02-27T12:07:36.809+0700] {subprocess.py:93} INFO - Setting default log level to "WARN".
[2024-02-27T12:07:36.810+0700] {subprocess.py:93} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2024-02-27T12:07:43.363+0700] {subprocess.py:93} INFO - 24/02/27 12:07:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-02-27T12:07:43.367+0700] {subprocess.py:93} INFO - 24/02/27 12:07:43 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2024-02-27T12:10:59.027+0700] {subprocess.py:93} INFO - 24/02/27 12:10:59 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 172.28.89.67:37043 in 10000 milliseconds
[2024-02-27T12:10:59.028+0700] {job.py:213} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.
server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 187, in heartbeat
    self._merge_from(Job._fetch_from_db(self, session))
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/jobs/job.py", line 308, in _fetch_from_db
    session.merge(job)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
    return self._merge(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
    merged = self.get(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2853, in get
    return self._get_impl(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
    return db_load_fn(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.
server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2024-02-27T12:10:59.283+0700] {job.py:221} ERROR - Job heartbeat failed with error. Scheduler is in unhealthy state
[2024-02-27T12:12:24.755+0700] {subprocess.py:93} INFO - [Stage 0:>                                                          (0 + 0) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                24/02/27 12:12:24 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2024-02-27T12:13:17.205+0700] {subprocess.py:93} INFO - [Stage 1:>                                                          (0 + 4) / 4]ERROR:root:Exception while sending command.
[2024-02-27T12:13:17.260+0700] {subprocess.py:93} INFO - Traceback (most recent call last):
[2024-02-27T12:13:17.260+0700] {subprocess.py:93} INFO -   File "/home/nhut/etl-***/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 516, in send_command
[2024-02-27T12:13:17.296+0700] {subprocess.py:93} INFO -     raise Py4JNetworkError("Answer from Java side is empty")
[2024-02-27T12:13:17.296+0700] {subprocess.py:93} INFO - py4j.protocol.Py4JNetworkError: Answer from Java side is empty
[2024-02-27T12:13:17.297+0700] {subprocess.py:93} INFO - 
[2024-02-27T12:13:17.297+0700] {subprocess.py:93} INFO - During handling of the above exception, another exception occurred:
[2024-02-27T12:13:17.297+0700] {subprocess.py:93} INFO - 
[2024-02-27T12:13:17.297+0700] {subprocess.py:93} INFO - Traceback (most recent call last):
[2024-02-27T12:13:17.298+0700] {subprocess.py:93} INFO -   File "/home/nhut/etl-***/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
[2024-02-27T12:13:17.298+0700] {subprocess.py:93} INFO -     response = connection.send_command(command)
[2024-02-27T12:13:17.298+0700] {subprocess.py:93} INFO -   File "/home/nhut/etl-***/.venv/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
[2024-02-27T12:13:17.299+0700] {subprocess.py:93} INFO -     raise Py4JNetworkError(
[2024-02-27T12:13:17.301+0700] {subprocess.py:93} INFO - py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2024-02-27T12:13:17.302+0700] {subprocess.py:93} INFO - Traceback (most recent call last):
[2024-02-27T12:13:17.303+0700] {subprocess.py:93} INFO -   File "/home/nhut/etl-***/pipeline/spark_ingestion_to_s3.py", line 31, in <module>
[2024-02-27T12:13:17.304+0700] {subprocess.py:93} INFO -     extract_data()
[2024-02-27T12:13:17.318+0700] {subprocess.py:93} INFO -   File "/home/nhut/etl-***/pipeline/spark_ingestion_to_s3.py", line 27, in extract_data
[2024-02-27T12:13:17.319+0700] {subprocess.py:93} INFO -     df.write.mode("overwrite").parquet("s3a://olistsalesbucket/" + name_file + ".parquet")
[2024-02-27T12:13:17.320+0700] {subprocess.py:93} INFO -   File "/home/nhut/etl-***/.venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1721, in parquet
[2024-02-27T12:13:17.320+0700] {subprocess.py:93} INFO -     self._jwrite.parquet(path)
[2024-02-27T12:13:17.321+0700] {subprocess.py:93} INFO -   File "/home/nhut/etl-***/.venv/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
[2024-02-27T12:13:17.322+0700] {subprocess.py:93} INFO -     return_value = get_return_value(
[2024-02-27T12:13:17.323+0700] {subprocess.py:93} INFO -   File "/home/nhut/etl-***/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
[2024-02-27T12:13:17.323+0700] {subprocess.py:93} INFO -     return f(*a, **kw)
[2024-02-27T12:13:17.333+0700] {subprocess.py:93} INFO -   File "/home/nhut/etl-***/.venv/lib/python3.10/site-packages/py4j/protocol.py", line 334, in get_return_value
[2024-02-27T12:13:17.334+0700] {subprocess.py:93} INFO -     raise Py4JError(
[2024-02-27T12:13:17.337+0700] {subprocess.py:93} INFO - py4j.protocol.Py4JError: An error occurred while calling o35.parquet
[2024-02-27T12:13:19.830+0700] {subprocess.py:97} INFO - Command exited with return code 1
[2024-02-27T12:13:21.983+0700] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/nhut/etl-airflow/.venv/lib/python3.10/site-packages/airflow/operators/bash.py", line 212, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2024-02-27T12:13:24.507+0700] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=missed_deadline_job_dag, task_id=spark_ingestion_to_s3, execution_date=20240226T112029, start_date=20240227T050712, end_date=20240227T051323
[2024-02-27T12:13:25.972+0700] {standard_task_runner.py:107} ERROR - Failed to execute job 144 for task spark_ingestion_to_s3 (Bash command failed. The command returned a non-zero exit code 1.; 32116)
[2024-02-27T12:13:26.491+0700] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-27T12:13:27.642+0700] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
